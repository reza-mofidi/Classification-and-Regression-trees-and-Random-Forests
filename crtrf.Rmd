---
title: "Decision Trees and Random Forest classifiers"
author: "R Mofidi"
date: "24/06/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Decision tree Analysis
Decision Trees are a class of tree like graph algorithms. In addition to being a predictive model decision trees use this tree like structure to illustrate the possible decisions and their consequences. It is one way of combining an algorithm and a decision support tool as well as a machine learning paradigm.  A decision tree consists of 3 constituents (no pun intended), a node which represents an attribute and a branch which represents the consequences of classifying the data set based on that attribute. Attributes can occur as a result of an active decision (a decision node, usually marked as a square), a chance node which is the consequences of an event outside of the decision makers’ control and an end node (leaf) which denotes one of the possible final outcomes of the decision tree algorithm. In this way, the findings of the decision tree can be illustrated in a compact and easy to follow format using an influence diagram which describes the relationship between actions and consequences.


In this way decision tree classifiers utilize the topography of a decision tree to map observations about a target item to conclusions about the value of the target item. Where the target value has a finite set of values (ordinal or nominal), the process is known as a classification tree. If the Target value is a continuous variable the resultant model is called regression tree. The umbrella term Classification and Regression Tree (CART) is used to group both processes. This class of classifiers were devised by the distinguished American statistician Leo Breiman who was also involved in their eventual evolution into random forest classifiers. The CARTs work from top down by choosing the variables which best splits a set of items into the intended classes.This is performed by calculating the Gini coefficient for the putative input variables. The Gini coefficient denotes the determinant ability of a variable in correctly classifying the data according to the output variable (which we are trying to predict). It is named after the Italian economist, sociologist and statistician Corrado Gini.  The algorithms created are simple to understand and require little preparation, It is possible to use categorical as well as continuous variables and create a white box model which is transparent to the user. It is possible to validate using statistical models such as a confusion matrix or receiver operator characteristic test.

Decision tree analysis involves a series of simple steps the first of which involves identifying and using the variable which best separates the dataset in accordance with the outcome of interest. This creates 2 leaves or children nodes. the remaining data set in each leaves is divided in a similar manner until the groups are  either too small or pure i.e. contain only a single outcome (pure). Some software programs create competing Trees and eliminate the ones which have lower predictive ability based on in sample testing. 


### The Fisher's or Anderson's iris dataset 
The following example is created using a commonly encountered database in R known as the Iris database. This database uses the width and length of the petal and sepal of iris flowers to classify the iris flowers into the 3 different species:
1- Setosa
2- Versicolor
3- 	Virginica

The following code describes how decision trees (classification and Regression Trees) can be developed in R. 

### Install the appropriate libraries and datasets

Clearly the first step in R involves downloading the appropriate packages required:

```{r}
data(iris)
library(caret)
library(ggplot2)
library(lattice)
head(iris)
```

### Separate training and test datasets

The next step involves partitioning the dataset into training and test datasets. This creates a training dataset for developing the tree and a testing dataset for cross validation:

```{r}
inTrain3<- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
trainingDS<- iris[inTrain3,]
testingDS<- iris[-inTrain3,]
dim (trainingDS); dim(testingDS)
```


### Make a plot to view data separation
It is very useful to see the separation between the data clusters as this can be used to predict whether the correct classifier has been selected for the task 


```{r, echo=FALSE}
qplot(Petal.Width, Sepal.Width, colour=Species, data=trainingDS)
```

### Developing the decision tree "rpart" model.
The following r code creates the decision tree classifier using the training dataset (trainingDS). This is saved as "modfit". Using the *rattle* package it is possible to  draw an aesthetically pleasing tree.  

```{r}
library(caret)
modfit<- train(Species~.,method="rpart", data=trainingDS)
print(modfit$finalModel)
```

### Plot the "modfit" classification Tree

```{r, echo=FALSE}
library(rattle)
fancyRpartPlot(modfit$finalModel)
```

### Cross validation by performing the prediction on the testing sample

Cross validation using testing dataset is the final step in forming a classifier. Strictly speaking it is not out of sample testing as the datset comes from the same dataset as the training sample. However as the training dataset has not been used to develop the model, cross validation is a valid replacement for out of sample testing. 
The following code performs the cross validation process:

```{r}
predict(modfit, newdata=testingDS)
PredR<- predict(modfit, newdata=testingDS)
summary(PredR)
```

### The Cross validation
```{r}
library(ggplot2)
qplot(Petal.Width, Petal.Length, colour=PredR,data=testingDS, main = "out of sample accuracy")
```

As you can see a simple CART classifier can accurately classify the iris database into the 3 different species based on the features included. In fact it is hard to see how the classifier can be improved upon without introducing new variables. 


Decision trees are used extensively as classiﬁers and decision aids in many areas including healthcare. They provide a mixture of simplicity, utility and functionality. For many uses classiﬁcation and regression trees are more than suﬃcient including many healthcare decision support models (2, 3). Random Forest classifiers which are discussed below represent the future of CART algorithms in the world of deep learning as data becomes cheaper and more freely available.


## Random Forests and Ensemble Classifiers

Ensemble Classifiers consist of multiple classifiers which by themselves may be weak (i.e. have low predictive ability) but combining them into an ensemble improves their predictive ability significantly.  A similar concept exists in statistics with the difference that machine learning the ensembles contain a finite set of constituent algorithms (and are used for practical machine learning), whilst a statistical ensemble is infinite and more of a theoretical construct. The design of this class of algorithms is based on the *ensemble theory* which states that a trained ensemble of algorithms can represent a single supervised trained algorithm. In general an ensemble algorithm functions better if it contains a diverse set of constituent algorithms trined on random selection of the data. 


In fact the individual constituents of the ensemble do not necessarily have to be weak classifiers, being part of the ensemble means that they do not need to be complex in structure which in turn helps protect against over-fitting (over-training). They key to a successful ensemble is “stochastic discrimination” which is discussed below. There are a few types of ensemble classifiers in existence including:
•	Bayes optimal classifiers
•	Bootstrap aggregating methods such as Random forest classifiers
•	Boosting: such as Adaboost 


In this paper we discuss random forest classifiers as they represent an extension of decision tree algorithms.

### Random Forest classifiers

An example of ensemble classifiers are random forests. Random forests were first designed by the American data scientist Tin Kam Ho in the 1990s. They are made up of an ensemble made up of (often a large) number of tree classifiers which are added together in order to improve their classification ability. It is a bootstrap aggregating method which means each of these decision tree classifiers contributes to the eventual decision with an equal weight. Each tree acts as a weak classifier and together a large number of trees form a random forest. The diversity of classifiers within the ensemble is the key to its performance. 


If you remember in order to develop an ensemble classifier two assumptions will need to be fulfilled. The second of these two assumptions was that each weak classifier should make its choice independently of the other classifiers. This concept is called “Stochastic discrimination”.  In order to fulfill this assumption each decision tree needs to undergo a different training process using a random selection of the training data set. This is why large, yet discrete datasets are ideal for random forests. This is a dilemma as often the training data is scarce.  One way of getting around this is randomization (random selection) of training data he used to train each tree classifier.  In order to develop a random forest during training process a number of hyperplanes are selected at random and are trained using a random sub set of they are available training data.  The final classification is performed using a majority vote system to perform the classification task. 

The following is an example of a small random forest classifier created using the iris database described above:

### Loading the required packages and database
```{r}
data(iris); library(ggplot2); library(randomForest);library(caret)
```

### Partitioning the data into training and testing sets
```{r}
inTrainRF<-createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training4<-iris[inTrainRF,]
testing4<-iris[-inTrainRF,]
```

### Training the random forest classifier:
This involves setting the output variable as the Species and the rest of the variables as input variables. 

```{r}
modFit<- train(Species~.,data=training4, method="rf", prox=TRUE)
modFit
```

As you can see the in sample accuracy of the data is excellent and it over performs most linear data models as well as classification and regression  trees (only marginally). Clearly concerns regarding over-training (over fitting) the data exists. This is why cross validation and data visualization is important. This is what the testing sample "testing4" is used for. 

Random forests can be complicated in order to understand their underlying anatomy, it is possible to view the constituent trees making up the random forest classifier:

```{r}
getTree(modFit$finalModel,k=2)
```

#### Visualizing the classifier

```{r}
irisP<- classCenter(training4[,c(3,4)], training4$Species, modFit$finalModel$prox)
irisP<-as.data.frame(irisP);irisP$Species<- rownames(irisP)
P<- qplot(Petal.Width, Petal.Length, col=Species, size=2,Shape=4,data=training4)
P+geom_point(aes(x=Petal.Width, y=Petal.Length, col=Species), size=2, shape=4, data=irisP)
```


### Making the prediction in the testing sample
The following R script codes for the process of cross validation of the classifier. 

```{r}
pred<-predict(modFit, testing4) 
testing4$predRight<-pred == testing4$Species
table(pred,testing4$Species)
```


The following plot illustrates the correct and incorrect classifications made by the random forest. 


```{r echo=FALSE}
qplot(Petal.Width, Petal.Length, colour=predRight,data=testing4, main = "out of sample accuracy of the Random Forest classifier")
```

Random forests are among the most powerful classifiers available and as you can see at a very basic level they are not too difficult to develop in R. Despite their complexity which can be a problem when dealing with large data sets, they are considered white-box classifiers. Well implemented random forests are more resilient to over fitting Random forests have less variance than single decision trees.

Complexity when large ensembles are being used to analyse large data sets has been the main impediment to their use. Which makes random forests a victim of their own success. 
This can be time consuming and complicated process in a world where there are many alternative machine learning algorithms. Because of this random forests are used less often than decision trees in healthcare informatics(5). I many ways healthcare informatics is behind the curve in implementing this class of classifiers as decision aids. 

## References

1- Lewis RJ. An introduction to classification and regression tree (CART) analysis. In Annual meeting of the society for academic emergency medicine in San Francisco, California 2000, (Vol. 14).

2- McBride OM, Mofidi R, Griffiths GD, Dawson AR, Chalmers RT, Stonebridge PA. Development of a decision tree to streamline infrainguinal vein graft surveillance. Annals of vascular surgery;36:182-9.

3- Mofidi R, McBride OM, Green BR, Gatenby T, Walker P, Milburn S. Validation of a decision tree to streamline infrainguinal vein graft surveillance. Annals of vascular surgery 2017;40:216-22.

4- Prasad AM, Iverson LR, Liaw A. Newer classification and regression tree techniques: bagging and random forests for ecological prediction. Ecosystems. 2006;9(2):181-99.

5- Masetic Z, Subasi A. Congestive heart failure detection using random forest classifier. Computer methods and programs in biomedicine. 2016;130:54-64.